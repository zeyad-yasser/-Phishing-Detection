{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kC4a6ZMiZPm",
        "outputId": "f13c9f3e-9fb1-49d1-9c67-878c12e1a267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-seYCFEqnYZcigH42YabKK3NbB_FwdXf\n",
            "To: /content/test_data.csv\n",
            "100% 4.94M/4.94M [00:00<00:00, 22.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YmZvmRZsjChHmKTkQpZxiHKfRc-jUdtP\n",
            "To: /content/training_data.csv\n",
            "100% 19.5M/19.5M [00:00<00:00, 50.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1-seYCFEqnYZcigH42YabKK3NbB_FwdXf\n",
        "!gdown 1YmZvmRZsjChHmKTkQpZxiHKfRc-jUdtP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGn3mTyAiPpw"
      },
      "source": [
        "# Text Classification with RoBERTa\n",
        "\n",
        "The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu et al. It is based on Googleâ€™s BERT model released in 2018: it modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n",
        "\n",
        "See:\n",
        "- RoBERTa's paper: https://arxiv.org/pdf/1907.11692.pdf\n",
        "- BERT's paper: https://arxiv.org/pdf/1810.04805.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9Qermi0iPpz",
        "outputId": "2deeee84-26d9-4705-a1ea-8b157618112e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "Transformers version: 4.31.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "import math\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tokenizers\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h48NIv1tiPp0",
        "outputId": "57e4b069-2c19-47da-b2dd-a67f91b7fbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of replicas: 1\n"
          ]
        }
      ],
      "source": [
        "# Detect hardware, return appropriate distribution strategy (you can see that it is pretty easy to set up).\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is set (always set in Kaggle)\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3txQHBwriPp1"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'roberta-base'\n",
        "MAX_LEN = 256\n",
        "ARTIFACTS_PATH = '../artifacts/'\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 3\n",
        "\n",
        "if not os.path.exists(ARTIFACTS_PATH):\n",
        "    os.makedirs(ARTIFACTS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whLeAUUbiPp2"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "Note that I have already cleaned the dataset, so the training should be better:\n",
        "- Remove punctuation symbols and double white spaces.\n",
        "- Lemmatization.\n",
        "- Remove stop words (see `spacy.lang.en.stop_words.STOP_WORDS`).\n",
        "- TD-IDF\n",
        "\n",
        "The reason of this preprocess is that I have used this dataset with other models, such as LSTM. However, I believe (I didn't test it) that RoBERTa could deal with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj_3Y4SpiPp3",
        "outputId": "14b9fc67-d5a1-43b0-9571-ab90bf0be8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of x_train_tfidf: (4166, 130635)\n",
            "Shape of x_test_tfidf: (1160, 130635)\n",
            "{'1', '0'}\n",
            "{'1', '0'}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Read the training data\n",
        "train_file_path = \"training_data.csv\"\n",
        "train_data = pd.read_csv(train_file_path, dtype=str)\n",
        "\n",
        "# Read the test data\n",
        "test_file_path = \"test_data.csv\"\n",
        "test_data = pd.read_csv(test_file_path, dtype=str)\n",
        "\n",
        "# Function to perform preprocessing on text data\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation symbols and double white spaces\n",
        "    text = ' '.join(text.split())  # Remove extra white spaces\n",
        "    text = ''.join([char for char in text if char.isalnum() or char == ' '])  # Remove punctuation\n",
        "    # Lemmatization\n",
        "    doc = nlp(text)\n",
        "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
        "    # Remove stop words\n",
        "    filtered_text = ' '.join([word for word in lemmatized_text.split() if word.lower() not in spacy.lang.en.stop_words.STOP_WORDS])\n",
        "    return filtered_text\n",
        "\n",
        "# Apply preprocessing to training data\n",
        "train_data['clean_text'] = train_data['text'].apply(preprocess_text)\n",
        "\n",
        "# Filter out rows with categories other than '0' and '1' from training data\n",
        "train_data = train_data[train_data['target'].isin(['0', '1'])]\n",
        "\n",
        "# Split the filtered training data into features and target\n",
        "x_train = train_data['clean_text'].to_numpy().reshape(-1)\n",
        "y_train = train_data['target'].to_numpy().reshape(-1)\n",
        "\n",
        "# Apply preprocessing to test data\n",
        "test_data['clean_text'] = test_data['text'].apply(preprocess_text)\n",
        "\n",
        "# Split the test data into features and target\n",
        "x_test = test_data['clean_text'].to_numpy().reshape(-1)\n",
        "y_test = test_data['target'].to_numpy().reshape(-1)\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "# Print the shape of the TF-IDF matrices\n",
        "print(\"Shape of x_train_tfidf:\", x_train_tfidf.shape)\n",
        "print(\"Shape of x_test_tfidf:\", x_test_tfidf.shape)\n",
        "print(set(y_test))\n",
        "print(set(y_train))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBZEoi6QiPp5"
      },
      "source": [
        "# Dataset analysis\n",
        "\n",
        "As you can see in the following plot, the samples **are not balanced**. This could cause problems during the training but, since they are not highly unbalanced, I have left it as it is.\n",
        "\n",
        "In other cases, such as fraud detecting where the positive classes are very few compared to the negative ones, we must apply techniques to balance it. For example, we could undersample the biggest category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3bUDuDdiPp7"
      },
      "source": [
        "In these histograms and stats, we can see that almost all texts contain $500$ or less words. Also, we can see that the average length is very different depending on the category.\n",
        "\n",
        "Please, note that the tokenization process may split words into several parts, so lengths could increase (or decrease too). This is only an orientative result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09FoHm0DiPp9"
      },
      "source": [
        "# Tokenize & encode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8a9ilkaiPp9"
      },
      "source": [
        "I am going to import the RoBERTa model from HuggingFace. Note that I must import not only the model but also the tokenizer (since I must use the same vocabulary as the model was trained with).\n",
        "\n",
        "We should take into account that RoBERTa's input accepts up-to 512 tokens, thus **we must truncate the tokenized texts**. In my case, I truncate to 256 tokens, but you can put a higher value in the variable `MAX_LEN`.\n",
        "\n",
        "See https://huggingface.co/roberta-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HDrP4G9iPp9"
      },
      "outputs": [],
      "source": [
        "def roberta_encode(texts, tokenizer):\n",
        "    ct = len(texts)\n",
        "    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n",
        "    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n",
        "    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n",
        "\n",
        "    for k, text in enumerate(texts):\n",
        "        # Tokenize\n",
        "        tok_text = tokenizer.tokenize(text)\n",
        "\n",
        "        # Truncate and convert tokens to numerical IDs\n",
        "        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n",
        "\n",
        "        input_length = len(enc_text) + 2\n",
        "        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n",
        "\n",
        "        # Add tokens [CLS] and [SEP] at the beginning and the end\n",
        "        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n",
        "\n",
        "        # Set to 1s in the attention input\n",
        "        attention_mask[k,:input_length] = 1\n",
        "\n",
        "    return {\n",
        "        'input_word_ids': input_ids,\n",
        "        'input_mask': attention_mask,\n",
        "        'input_type_ids': token_type_ids\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un2FM-dJiPp-",
        "outputId": "debc4953-c611-46e0-9511-892074d25b76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '1', 1: '0'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transform categories into numbers\n",
        "category_to_id = {}\n",
        "category_to_name = {}\n",
        "\n",
        "for index, c in enumerate(y_train):\n",
        "    if c in category_to_id:\n",
        "        category_id = category_to_id[c]\n",
        "    else:\n",
        "        category_id = len(category_to_id)\n",
        "        category_to_id[c] = category_id\n",
        "        category_to_name[category_id] = c\n",
        "\n",
        "    y_train[index] = category_id\n",
        "\n",
        "# Display dictionary\n",
        "category_to_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRKfsiA-iPp-"
      },
      "outputs": [],
      "source": [
        "# Split into train and test datasets\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=777) # random_state to reproduce results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJwf1c9yiPp_"
      },
      "outputs": [],
      "source": [
        "# Import tokenizer from HuggingFace\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SWjVnNbiPp_"
      },
      "outputs": [],
      "source": [
        "X_train_encoded = roberta_encode(X_train, tokenizer)\n",
        "X_val_encoded = roberta_encode(X_val, tokenizer)\n",
        "x_test_encoded = roberta_encode(x_test, tokenizer)\n",
        "\n",
        "\n",
        "y_train = np.asarray(y_train, dtype='int32')\n",
        "y_val = np.asarray(y_val, dtype='int32')\n",
        "y_test = np.asarray(y_test, dtype='int32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4iUk-dJiPp_"
      },
      "source": [
        "## Create RoBERTa model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP14ueLsiPqA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def build_model(n_categories):\n",
        "    with strategy.scope():\n",
        "        input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "        input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
        "\n",
        "        input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
        "        print(input_word_ids)\n",
        "        print(input_mask)\n",
        "        print(input_type_ids)\n",
        "\n",
        "        # Import RoBERTa model from HuggingFace\n",
        "        roberta_model = TFRobertaModel.from_pretrained(MODEL_NAME,trainable=True)\n",
        "        x = roberta_model(input_word_ids, attention_mask=input_mask)\n",
        "        print(x)\n",
        "\n",
        "        # Huggingface transformers have multiple outputs, embeddings are the first one,\n",
        "        # so let's slice out the first position\n",
        "        x = x[0]\n",
        "\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "        x = tf.keras.layers.Flatten()(x)\n",
        "        x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dense(1, activation='softmax')(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYHR_PW0iPqA",
        "outputId": "40677aeb-c7c9-4df4-e8b8-53a4438849f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.int32, name='input_word_ids'), name='input_word_ids', description=\"created by layer 'input_word_ids'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.int32, name='input_mask'), name='input_mask', description=\"created by layer 'input_mask'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.int32, name='input_type_ids'), name='input_type_ids', description=\"created by layer 'input_type_ids'\")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.embeddings.position_ids', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<KerasTensor: shape=(None, 256, 768) dtype=float32 (created by layer 'tf_roberta_model')>, pooler_output=<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_roberta_model')>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer  [(None, 256)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)     [(None, 256)]                0         []                            \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobert  TFBaseModelOutputWithPooli   1246456   ['input_word_ids[0][0]',      \n",
            " aModel)                     ngAndCrossAttentions(last_   32         'input_mask[0][0]']          \n",
            "                             hidden_state=(None, 256, 7                                           \n",
            "                             68),                                                                 \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)        (None, 256, 768)             0         ['tf_roberta_model[0][0]']    \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 196608)               0         ['dropout_37[0][0]']          \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  5033190   ['flatten[0][0]']             \n",
            "                                                          4                                       \n",
            "                                                                                                  \n",
            " input_type_ids (InputLayer  [(None, 256)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1)                    257       ['dense[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 174977793 (667.49 MB)\n",
            "Trainable params: 174977793 (667.49 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "n_categories = 2  # Number of categories/classes\n",
        "\n",
        "with strategy.scope():\n",
        "    model = build_model(n_categories)\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP2GItEliPqA"
      },
      "source": [
        "## Train model\n",
        "\n",
        "This is basic training of RoBERTa but, if your dataset is larger, you may use K-Folds in this section. In this notebook, I use K-Folds (use it as inspiration): https://www.kaggle.com/dimasmunoz/clean-english-data-roberta ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "hK5YIINcGoEb",
        "outputId": "e2c0f19c-bd40-4683-dbad-1c84a8bf1bd6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_encoded' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0881e6754fd3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print shapes of input data and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of X_train_encoded:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_word_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of X_val_encoded:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_word_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of test samples: {X_test_encoded.shape[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_encoded' is not defined"
          ]
        }
      ],
      "source": [
        "# Print shapes of input data and labels\n",
        "print(\"Shape of X_train_encoded:\", X_train_encoded['input_word_ids'].shape)\n",
        "print(\"Shape of X_val_encoded:\", X_val_encoded['input_word_ids'].shape)\n",
        "print(f\"Number of test samples: {X_test_encoded.shape[0]}\")\n",
        "\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "\n",
        "# Print a few samples of input data and labels\n",
        "print(\"Sample of X_train_encoded:\", X_train_encoded['input_word_ids'][0])\n",
        "print(\"Sample of y_train:\", y_train[0])\n",
        "\n",
        "# Check for NaN or missing values\n",
        "print(\"NaN values in X_train_encoded:\", np.isnan(X_train_encoded['input_word_ids']).any())\n",
        "print(\"NaN values in X_val_encoded:\", np.isnan(X_val_encoded['input_word_ids']).any())\n",
        "print(\"NaN values in y_train:\", np.isnan(y_train).any())\n",
        "\n",
        "# Check label encoding\n",
        "print(\"Unique labels in y_train:\", np.unique(y_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbQThSdpiPqA",
        "outputId": "8d32521b-5c4a-4533-ad9b-16af66df186d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "235/235 [==============================] - 12790s 54s/step - loss: 2.8816 - accuracy: 0.6743 - val_loss: 1.8248 - val_accuracy: 0.6667\n",
            "Epoch 2/5\n",
            "100/235 [===========>..................] - ETA: 1:56:49 - loss: 1.3796 - accuracy: 0.6731"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    print('Training...')\n",
        "    history = model.fit(X_train_encoded,\n",
        "                        y_train,\n",
        "                        epochs=5,\n",
        "                        batch_size=16,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_val_encoded, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rROUa5PdiPqB",
        "outputId": "6a85401c-bf9f-4449-e73f-43c91f3ee218"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6ac9d60c7c65>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mxaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# This plot will look much better if we train models with more epochs, but anyway here is\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.title('Accuracy')\n",
        "\n",
        "xaxis = np.arange(len(history.history['accuracy']))\n",
        "plt.plot(xaxis, history.history['accuracy'], label='Train set')\n",
        "plt.plot(xaxis, history.history['val_accuracy'], label='Validation set')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSTFcxLEiPqB"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "In a confusion matrix, we can see how many categories are classified c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMmPDPIOiPqB"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(X_test, y_test, model):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = [np.argmax(i) for i in model.predict(X_test)]\n",
        "\n",
        "    con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()\n",
        "\n",
        "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "    label_names = list(range(len(con_mat_norm)))\n",
        "\n",
        "    con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                              index=label_names,\n",
        "                              columns=label_names)\n",
        "\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    sns.heatmap(con_mat_df, cmap=plt.cm.Blues, annot=True)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROZKh_AaiPqC"
      },
      "outputs": [],
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aka5b8PiPqC"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(x_test, y_test, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS3IF5XdiPqC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}